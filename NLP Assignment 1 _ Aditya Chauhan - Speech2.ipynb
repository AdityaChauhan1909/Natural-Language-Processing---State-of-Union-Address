{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycorpus= PlaintextCorpusReader('.', '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_union_part2 = mycorpus.raw('state_union_part2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of Complete State of the Union Addresses,\\r\\nfrom 1946 to the Present\\r\\n(#4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_union_part2[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621720"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Determining the length of the document 1 ( number of alphabets)\n",
    "\n",
    "len(state_union_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5954"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_union_part2.rfind(\"Barack Obama, State of the Union Address, January 12, 2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State of the Union Address\n",
      " 127\n",
      "State of the Union Address\n",
      " 3418\n",
      "State of the Union Address\n",
      " 4157\n",
      "State of the Union Address\n",
      " 4792\n",
      "State of the Union Address\n",
      " 5221\n",
      "State of the Union Address\n",
      " 5778\n",
      "State of the Union Address\n",
      " 6261\n",
      "State of the Union Address\n",
      " 6864\n",
      "State of the Union Address\n",
      " 7926\n",
      "State of the Union Address\n",
      " 8800\n",
      "State of the Union Address\n",
      " 9544\n",
      "State of the Union Address\n",
      " 10404\n",
      "State of the Union Address\n",
      " 11353\n",
      "State of the Union Address\n",
      " 11845\n",
      "State of the Union Address\n",
      " 12494\n",
      "State of the Union Address\n",
      " 13116\n",
      "State of the Union Address\n",
      " 13815\n",
      "State of the Union Address\n",
      " 14624\n",
      "State of the Union Address\n",
      " 15200\n",
      "State of the Union Address\n",
      " 15951\n",
      "State of the Union Address\n",
      " 16548\n",
      "State of the Union Address\n",
      " 16931\n",
      "State of the Union Address\n",
      " 17592\n",
      "State of the Union Address\n",
      " 18249\n",
      "State of the Union Address\n",
      " 19105\n",
      "State of the Union Address\n",
      " 19770\n",
      "State of the Union Address\n",
      " 20282\n",
      "State of the Union Address\n",
      " 20831\n",
      "State of the Union Address\n",
      " 21382\n",
      "State of the Union Address\n",
      " 21883\n",
      "State of the Union Address\n",
      " 22087\n",
      "State of the Union Address\n",
      " 22640\n",
      "State of the Union Address\n",
      " 23137\n",
      "State of the Union Address\n",
      " 23741\n",
      "State of the Union Address\n",
      " 24250\n",
      "State of the Union Address\n",
      " 24796\n",
      "State of the Union Address\n",
      " 25189\n",
      "State of the Union Address\n",
      " 25567\n",
      "State of the Union Address\n",
      " 29704\n",
      "State of the Union Address\n",
      " 30272\n",
      "State of the Union Address\n",
      " 30874\n",
      "State of the Union Address\n",
      " 31425\n",
      "State of the Union Address\n",
      " 31904\n",
      "State of the Union Address\n",
      " 32253\n",
      "State of the Union Address\n",
      " 32628\n",
      "State of the Union Address\n",
      " 33112\n",
      "State of the Union Address\n",
      " 33542\n",
      "State of the Union Address\n",
      " 33997\n",
      "State of the Union Address\n",
      " 34458\n",
      "State of the Union Address\n",
      " 35234\n",
      "State of the Union Address\n",
      " 36271\n",
      "State of the Union Address\n",
      " 36974\n",
      "State of the Union Address\n",
      " 37735\n",
      "State of the Union Address\n",
      " 38546\n",
      "State of the Union Address\n",
      " 39416\n",
      "State of the Union Address\n",
      " 40263\n",
      "State of the Union Address\n",
      " 40753\n",
      "State of the Union Address\n",
      " 41098\n",
      "State of the Union Address\n",
      " 41536\n",
      "State of the Union Address\n",
      " 42116\n",
      "State of the Union Address\n",
      " 42675\n",
      "State of the Union Address\n",
      " 43205\n"
     ]
    }
   ],
   "source": [
    "#The actual content starts after the list of names of the presidents who delivered the speech along \n",
    "#with the year they delivered the speech in.\n",
    "\n",
    "# I searched for the pattern where the line starts with \"State of the Union Address\" and also\n",
    "# instructed python to give me the line number, so that we can start analyzing the text from that line(actual content)\n",
    "\n",
    "import re\n",
    "occurance_state_union_part2 = open(\"state_union_part2.txt\", \"r\")\n",
    "regex = re.compile('(S|s)tate of the Union Address')\n",
    "\n",
    "for line_no, line in enumerate(occurance_state_union_part2):\n",
    "    if re.match(regex, line):\n",
    "        print(line, line_no)\n",
    "occurance_state_union_part2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['State of the Union Address\\n',\n",
       " 'Harry S. Truman\\n',\n",
       " 'January 21, 1946\\n',\n",
       " '\\n',\n",
       " 'To the Congress of the United States:\\n',\n",
       " '\\n',\n",
       " 'A quarter century ago the Congress decided that it could no longer\\n',\n",
       " 'consider the financial programs of the various departments on a\\n',\n",
       " 'piecemeal basis. Instead it has called on the President to present a\\n',\n",
       " 'comprehensive Executive Budget. The Congress has shown its satisfaction\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering the content\n",
    "state_union_profile2 = open(\"state_union_part2.txt\", \"r\")\n",
    "state_union_part2_filtered = state_union_profile2.readlines()[127:]\n",
    "state_union_part2_filtered[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Closing the file\n",
    "state_union_profile2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Writing the content to other notepad file\n",
    "state_union_part2_filtered = ''.join(state_union_part2_filtered)\n",
    "file = open(\"state_union_part2_filtered.txt\",\"w\")\n",
    "file.write(state_union_part2_filtered)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycorpus= PlaintextCorpusReader('.', '.*\\.txt')\n",
    "state_union_part2_filtered_ = mycorpus.raw('state_union_part2_filtered.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'State of t'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_union_part2_filtered_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2615695"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_union_part2_filtered_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_union_part2_filtered_tokens = word_tokenize(state_union_part2_filtered_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['State',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Union',\n",
       " 'Address',\n",
       " 'Harry',\n",
       " 'S.',\n",
       " 'Truman',\n",
       " 'January',\n",
       " '21']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_union_part2_filtered_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483009"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_union_part2_filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the content to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Changing the content to lowercase\n",
    "state_union_part2_filtered_tokens = [w.lower() for w in state_union_part2_filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'union',\n",
       " 'address',\n",
       " 'harry',\n",
       " 's.',\n",
       " 'truman',\n",
       " 'january',\n",
       " '21']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_union_part2_filtered_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing punctuations\n",
    "import string \n",
    "string.punctuation\n",
    "state_union_part2_filtered_tokens_nopunc =  [ w for w in state_union_part2_filtered_tokens if w not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'union',\n",
       " 'address',\n",
       " 'harry',\n",
       " 's.',\n",
       " 'truman',\n",
       " 'january',\n",
       " '21']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_union_part2_filtered_tokens_nopunc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Cleaning the dataset . Removing numbers and punctuations\n",
    "I have also created another dataset with no numbers and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the dataset . Removing numbers and punctuations\n",
    "state_union_part2_filtered_tokens_clean = [ w for w in state_union_part2_filtered_tokens if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'union',\n",
       " 'address',\n",
       " 'harry',\n",
       " 'truman',\n",
       " 'january',\n",
       " 'to',\n",
       " 'the']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_union_part2_filtered_tokens_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_union_part2_filtered_tokens_clean_stop = [ w for w in state_union_part2_filtered_tokens_clean if w not in stop_words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202911"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of stop words\n",
    "state_union_part2_stopwordcount = [ w for w in state_union_part2_filtered_tokens_clean if w in stop_words ]\n",
    "len(state_union_part2_stopwordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219783"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_union_part2_filtered_tokens_clean_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing similar function on dataset with numbers but with no punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state',\n",
       " 'union',\n",
       " 'address',\n",
       " 'harry',\n",
       " 's.',\n",
       " 'truman',\n",
       " 'january',\n",
       " '21',\n",
       " '1946',\n",
       " 'congress']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_union_part2_filtered_tokens_nopunc_stop =  [ w for w in state_union_part2_filtered_tokens_nopunc if w not in stop_words]\n",
    "state_union_part2_filtered_tokens_nopunc_stop[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution of Speech Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Frequency Distribution\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fdist2 = FreqDist(state_union_part2_filtered_tokens_clean_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('must', 1628),\n",
       " ('people', 1506),\n",
       " ('world', 1488),\n",
       " ('new', 1441),\n",
       " ('america', 1271),\n",
       " ('year', 1265),\n",
       " ('congress', 1230),\n",
       " ('us', 1215),\n",
       " ('government', 1111),\n",
       " ('years', 1111),\n",
       " ('american', 950),\n",
       " ('nation', 861),\n",
       " ('one', 804),\n",
       " ('every', 780),\n",
       " ('make', 777),\n",
       " ('work', 754),\n",
       " ('federal', 744),\n",
       " ('time', 741),\n",
       " ('states', 711),\n",
       " ('americans', 688),\n",
       " ('help', 686),\n",
       " ('security', 685),\n",
       " ('war', 674),\n",
       " ('economic', 671),\n",
       " ('peace', 668),\n",
       " ('united', 651),\n",
       " ('nations', 645),\n",
       " ('program', 638),\n",
       " ('also', 638),\n",
       " ('country', 629),\n",
       " ('national', 609),\n",
       " ('economy', 588),\n",
       " ('great', 583),\n",
       " ('last', 572),\n",
       " ('many', 563),\n",
       " ('free', 557),\n",
       " ('need', 554),\n",
       " ('first', 552),\n",
       " ('let', 549),\n",
       " ('would', 548),\n",
       " ('tax', 514),\n",
       " ('know', 507),\n",
       " ('million', 507),\n",
       " ('freedom', 503),\n",
       " ('budget', 501),\n",
       " ('health', 489),\n",
       " ('future', 475),\n",
       " ('system', 463),\n",
       " ('programs', 462),\n",
       " ('tonight', 461)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fdist2.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219783"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_union_part2_filtered_tokens_clean_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('must', 0.007407306297575336)\n",
      "('people', 0.006852213319501508)\n",
      "('world', 0.006770314355523403)\n",
      "('new', 0.006556467060691682)\n",
      "('america', 0.005782976845342906)\n",
      "('year', 0.005755677190683538)\n",
      "('congress', 0.005596429205170555)\n",
      "('us', 0.005528180068522133)\n",
      "('government', 0.005054986054426411)\n",
      "('years', 0.005054986054426411)\n",
      "('american', 0.004322445321066688)\n",
      "('nation', 0.003917500443619389)\n",
      "('one', 0.003658153724355387)\n",
      "('every', 0.0035489551057179126)\n",
      "('make', 0.0035353052783882286)\n",
      "('work', 0.0034306566021939824)\n",
      "('federal', 0.0033851571777617014)\n",
      "('time', 0.003371507350432017)\n",
      "('states', 0.003235009077135174)\n",
      "('americans', 0.003130360400940928)\n",
      "('help', 0.003121260516054472)\n",
      "('security', 0.003116710573611244)\n",
      "('war', 0.003066661206735735)\n",
      "('economic', 0.0030530113794060504)\n",
      "('peace', 0.0030393615520763664)\n",
      "('united', 0.0029620125305414888)\n",
      "('nations', 0.00293471287588212)\n",
      "('program', 0.0029028632787795237)\n",
      "('also', 0.0029028632787795237)\n",
      "('country', 0.0028619137967904706)\n",
      "('national', 0.0027709149479259085)\n",
      "('economy', 0.002675366156618119)\n",
      "('great', 0.0026526164444019783)\n",
      "('last', 0.0026025670775264693)\n",
      "('many', 0.0025616175955374167)\n",
      "('free', 0.0025343179408780477)\n",
      "('need', 0.0025206681135483636)\n",
      "('first', 0.0025115682286619076)\n",
      "('let', 0.002497918401332223)\n",
      "('would', 0.002493368458888995)\n",
      "('tax', 0.00233867041581924)\n",
      "('know', 0.0023068208187166433)\n",
      "('million', 0.0023068208187166433)\n",
      "('freedom', 0.002288621048943731)\n",
      "('budget', 0.0022795211640572748)\n",
      "('health', 0.0022249218547385377)\n",
      "('future', 0.002161222660533344)\n",
      "('system', 0.002106623351214607)\n",
      "('programs', 0.002102073408771379)\n",
      "('tonight', 0.002097523466328151)\n"
     ]
    }
   ],
   "source": [
    "#Normalizing the most common words by length of document\n",
    "for w in Fdist2.most_common(50):\n",
    "    normalized_freq_part2 = (w[0], w[1]/len(state_union_part2_filtered_tokens_clean_stop))\n",
    "    print(normalized_freq_part2)\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 ( b)  - Bigram by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(state_union_part2_filtered_tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('of', 'the'), 0.007523172791664892)\n",
      "(('in', 'the'), 0.005900249352959824)\n",
      "(('to', 'the'), 0.0033404779817078076)\n",
      "(('of', 'our'), 0.003189068214831533)\n",
      "(('and', 'the'), 0.0025597713712520167)\n",
      "(('for', 'the'), 0.002410727381983184)\n",
      "(('we', 'have'), 0.002335022498545047)\n",
      "(('we', 'must'), 0.002185978509276214)\n",
      "(('the', 'world'), 0.0019706927469990113)\n",
      "(('the', 'congress'), 0.0016844336564985545)\n",
      "(('will', 'be'), 0.0014951714479032113)\n",
      "(('we', 'are'), 0.0014691478942213516)\n",
      "(('it', 'is'), 0.0014502216733618173)\n",
      "(('we', 'can'), 0.0014431243405394918)\n",
      "(('on', 'the'), 0.00144075856293205)\n",
      "(('the', 'united'), 0.0013626879018864711)\n",
      "(('we', 'will'), 0.0013390301258120532)\n",
      "(('with', 'the'), 0.001329567015382286)\n",
      "(('and', 'to'), 0.0012940803512706592)\n",
      "(('in', 'our'), 0.0012775199080185666)\n",
      "(('that', 'the'), 0.0012443990215143816)\n",
      "(('by', 'the'), 0.00121600969022508)\n",
      "(('that', 'we'), 0.0011474021396092682)\n",
      "(('united', 'states'), 0.001092989254638107)\n",
      "(('and', 'we'), 0.0010764288113860145)\n",
      "(('in', 'this'), 0.001071697256171131)\n",
      "(('to', 'be'), 0.0010575025905264802)\n",
      "(('more', 'than'), 0.00099126081751811)\n",
      "(('in', 'a'), 0.0009747003742660175)\n",
      "(('of', 'a'), 0.000969968819051134)\n",
      "(('and', 'i'), 0.0009676030414436922)\n",
      "(('has', 'been'), 0.0009605057086213667)\n",
      "(('is', 'the'), 0.0009155559340799727)\n",
      "(('have', 'been'), 0.000901361268435322)\n",
      "(('of', 'this'), 0.0008848008251832295)\n",
      "(('the', 'american'), 0.0008682403819311369)\n",
      "(('a', 'new'), 0.0008422168282492772)\n",
      "(('i', 'have'), 0.0008019986089227669)\n",
      "(('is', 'a'), 0.0007759750552409072)\n",
      "(('must', 'be'), 0.0007759750552409072)\n",
      "(('to', 'make'), 0.0007688777224185817)\n",
      "(('from', 'the'), 0.0007641461672036982)\n",
      "(('as', 'a'), 0.0007475857239516056)\n",
      "(('to', 'help'), 0.0007475857239516056)\n",
      "(('at', 'the'), 0.0007191963926623042)\n",
      "(('the', 'people'), 0.0007168306150548624)\n",
      "(('can', 'not'), 0.0006813439509432356)\n",
      "(('if', 'we'), 0.0006813439509432356)\n",
      "(('to', 'our'), 0.0006813439509432356)\n",
      "(('and', 'our'), 0.0006766123957283519)\n"
     ]
    }
   ],
   "source": [
    "for bscore in scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 ( c)  - Bigram by mutual information score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(5)\n",
    "scored_pmi = finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('el', 'salvador'), 16.10429160765597)\n",
      "(('bin', 'laden'), 15.881899186319526)\n",
      "(('saudi', 'arabia'), 15.881899186319522)\n",
      "(('sam', 'rayburn'), 15.689254108377124)\n",
      "(('barack', 'obama'), 15.519329106934816)\n",
      "(('lyndon', 'johnson'), 15.229822489739831)\n",
      "(('northern', 'ireland'), 15.104291607655973)\n",
      "(('floor', 'appears'), 14.95228851421092)\n",
      "(('iron', 'curtain'), 14.881899186319522)\n",
      "(('grass', 'roots'), 14.841257201822177)\n",
      "(('william', 'clinton'), 14.796169312293642)\n",
      "(('ronald', 'reagan'), 14.782363512768608)\n",
      "(('thomas', 'jefferson'), 14.72577998440224)\n",
      "(('red', 'tape'), 14.689254108377126)\n",
      "(('jill', 'biden'), 14.61886478048573)\n",
      "(('teen', 'pregnancy'), 14.519329106934816)\n",
      "(('abraham', 'lincoln'), 14.431866265684477)\n",
      "(('empowerment', 'zones'), 14.296936685598364)\n",
      "(('synthetic', 'fuels'), 14.215322920044716)\n",
      "(('dwight', 'eisenhower'), 14.004755934105058)\n",
      "(('intercontinental', 'ballistic'), 13.943299730983664)\n",
      "(('harry', 'truman'), 13.831273113249555)\n",
      "(('status', 'quo'), 13.831273113249555)\n",
      "(('river', 'basins'), 13.831273113249553)\n",
      "(('prime', 'minister'), 13.782363512768608)\n",
      "(('nationwide', 'radio'), 13.741721528271263)\n",
      "(('richard', 'nixon'), 13.589718434826214)\n",
      "(('al', 'qaeda'), 13.559971091432162)\n",
      "(('al', 'qaida'), 13.55997109143216)\n",
      "(('saddam', 'hussein'), 13.519329106934816)\n",
      "(('george', 'bush'), 13.48631404970685)\n",
      "(('supreme', 'court'), 13.465252434179021)\n",
      "(('carbon', 'pollution'), 13.409146189184392)\n",
      "(('baby', 'boom'), 13.403851889514879)\n",
      "(('persian', 'gulf'), 13.377196257513825)\n",
      "(('indian', 'ocean'), 13.267790339938852)\n",
      "(('panama', 'canal'), 13.116364439956545)\n",
      "(('displaced', 'persons'), 13.051824187761836)\n",
      "(('per', 'capita'), 13.045397918602402)\n",
      "(('franklin', 'roosevelt'), 13.016828766405633)\n",
      "(('hardest', 'hit'), 12.907894394852466)\n",
      "(('wall', 'street'), 12.867252410355123)\n",
      "(('marriage', 'penalty'), 12.841257201822177)\n",
      "(('steam', 'coal'), 12.841257201822177)\n",
      "(('unliquidated', 'obligations'), 12.72577998440224)\n",
      "(('catastrophic', 'illness'), 12.71197418487721)\n",
      "(('distinguished', 'guests'), 12.577044604791102)\n",
      "(('raw', 'materials'), 12.563723226293268)\n",
      "(('ballistic', 'missiles'), 12.544203775573843)\n",
      "(('contract', 'authorizations'), 12.542412720047857)\n"
     ]
    }
   ],
   "source": [
    "for bscore in scored_pmi[:50]:\n",
    "    \n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Answer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#There were 41146 punctuations in state_union_part1 and \n",
    "# 556723 Tokens in  Text 1\n",
    "#Tokens in 483009 tokens in text 2\n",
    "#Text 2 Punctuations: 46274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.390749079883532"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of punctuations in Text 1\n",
    "(41146/556723)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.58035978625657"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of punctuations in Text 2\n",
    "(46274/483009)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Both the texts talks about the current affairs of that time. Text1 contains bigrams more of Latin Origin\n",
    "# and discuss about places such as San Francisco, Guadalupe Hidalgo, Porto Rico etc.\n",
    "\n",
    "# Text 2 is more concentrated towards words related to terrorism and negativity. \n",
    "#This includes examples such as : Bin Laden, Ballistic Missiles, catastrophic illness, Al Qaida etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.00977621535002"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of tokens: 483009\n",
    "#Number of Stop words in Text2 : 202911\n",
    "(202911/483009) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.878347041526936"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of tokens:556723\n",
    "#Number of stop words in text1 : 272117\n",
    "(272117/556723) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
